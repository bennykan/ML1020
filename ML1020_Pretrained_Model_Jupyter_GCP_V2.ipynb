{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/bennykan/ML1020/blob/master/Ml1020_Pretrained_Model_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===========================================================\n",
    "# ML1020 - Final Project: Mid-Term Proposal\n",
    "# Team Blue\n",
    "\n",
    "## Tyler Blakeley\n",
    "## Benjamin Kan\n",
    "## Avi Singh\n",
    "## Justin Kim\n",
    "\n",
    "\n",
    "# Distracted Driver Detection\n",
    "# ==========================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project is inspired by a past Kaggle competition hosted by State Farm, an insurance company based in the US.  Competition participants were invited to implement a machine learning algorithm to classify and predict the driverâ€™s behavior based on the images captured from the dashboard cameras installed in the vehicles (https://www.kaggle.com/c/state-farm-distracted-driver-detection/data). The host defined the following 10 driving behavior classifications\n",
    "\n",
    "| Label | Driver Behavior Descriptions |\n",
    "| ----- | ---------------------------- |\n",
    "| c0\t| normal driving\n",
    "| c1\t| texting - right\n",
    "| c2\t| talking on the phone - right\n",
    "| c3\t| texting - left\n",
    "| c4\t| talking on the phone - left\n",
    "| c5\t| operating the radio\n",
    "| c6\t| drinking\n",
    "| c7\t| reaching behind\n",
    "| c8\t| hair and makeup\n",
    "| c9\t| talking to passenger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Keras Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c69DxfPUv00z"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.version\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/ML1020\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "z4Eu93PNytge",
    "outputId": "0c6726bb-43e9-442c-d1ad-784c66f73234"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/Data/train\n",
      "Found 22424 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "base_dir = '/home/jupyter/Data'\n",
    "\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "print(train_dir)\n",
    "#validation_dir = os.path.join(base_dir, 'validation')\n",
    "#test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "batch_size = 10\n",
    "\n",
    "train_features = np.zeros(shape=(22424, 4, 4, 512))\n",
    "train_labels = np.zeros(shape=(22424,10))\n",
    " \n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(480, 640),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "colab_type": "code",
    "id": "zcLWhSFg91rw",
    "outputId": "bc535b30-480c-4f60-f295-5c96ef1925ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img = cv2.imread(base_dir + '/train/c6/img_380.jpg')\n",
    "img_cvt=cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img_cvt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the VGG16 Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "MoJLVH6WDo8e",
    "outputId": "ea22c8b3-66c5-42d4-d40f-b1a20d10fb10"
   },
   "outputs": [],
   "source": [
    "#Loading in Pretrained Model\n",
    "\n",
    "from keras.applications import VGG16\n",
    "\n",
    "conv_base = VGG16(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(150, 150, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 782
    },
    "colab_type": "code",
    "id": "pGlhU7kzBMKy",
    "outputId": "b6f8028c-2857-4f85-ef17-4c7aacc66687"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 150, 150, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding One Hidden Layer on Top of VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 4, 4, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               2097408   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 16,814,666\n",
      "Trainable params: 16,814,666\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Model with Conv Base Included\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "model = models.Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu', input_dim=4 * 4 * 512))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(10, activation = 'softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "#Set conv_base training weights to false so we dont re train the weights already learned\n",
    "print(len(model.trainable_weights))\n",
    "conv_base.trainable = False\n",
    "print(len(model.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the Last Convolution Layer in VGG16 to be Re-Trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "conv_base.trainable = True\n",
    "\n",
    "set_trainable = False\n",
    "for layer in conv_base.layers:\n",
    "    if layer.name == 'block5_conv1':\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "print(len(model.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22424 images belonging to 10 classes.\n",
      "Epoch 1/20\n",
      " - 75s - loss: 2.4885 - acc: 0.1075\n",
      "Epoch 2/20\n",
      " - 59s - loss: 2.3421 - acc: 0.1300\n",
      "Epoch 3/20\n",
      " - 60s - loss: 2.2728 - acc: 0.1535\n",
      "Epoch 4/20\n",
      " - 60s - loss: 2.2062 - acc: 0.1860\n",
      "Epoch 5/20\n",
      " - 61s - loss: 2.1559 - acc: 0.2115\n",
      "Epoch 6/20\n",
      " - 61s - loss: 2.0695 - acc: 0.2660\n",
      "Epoch 7/20\n",
      " - 62s - loss: 1.9949 - acc: 0.2830\n",
      "Epoch 8/20\n",
      " - 62s - loss: 1.8623 - acc: 0.3370\n",
      "Epoch 9/20\n",
      " - 62s - loss: 1.8019 - acc: 0.3635\n",
      "Epoch 10/20\n",
      " - 65s - loss: 1.7373 - acc: 0.3730\n",
      "Epoch 11/20\n",
      " - 66s - loss: 1.6203 - acc: 0.4170\n",
      "Epoch 12/20\n",
      " - 27s - loss: 1.5457 - acc: 0.4450\n",
      "Epoch 13/20\n",
      " - 17s - loss: 1.5102 - acc: 0.4540\n",
      "Epoch 14/20\n",
      " - 17s - loss: 1.4452 - acc: 0.4940\n",
      "Epoch 15/20\n",
      " - 17s - loss: 1.3868 - acc: 0.5130\n",
      "Epoch 16/20\n",
      " - 17s - loss: 1.2859 - acc: 0.5535\n",
      "Epoch 17/20\n",
      " - 17s - loss: 1.2487 - acc: 0.5770\n",
      "Epoch 18/20\n",
      " - 17s - loss: 1.1828 - acc: 0.5890\n",
      "Epoch 19/20\n",
      " - 17s - loss: 1.1635 - acc: 0.5985\n",
      "Epoch 20/20\n",
      " - 17s - loss: 1.0877 - acc: 0.6255\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "base_dir = '/home/jupyter/Data'\n",
    "\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # This is the target directory\n",
    "        train_dir,\n",
    "        # All images will be resized to 150x150\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='categorical')\n",
    "\n",
    "#validation_generator = test_datagen.flow_from_directory(\n",
    "        #validation_dir,\n",
    "        #target_size=(150, 150),\n",
    "        #batch_size=20,\n",
    "        #class_mode='binary')\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-5),\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,\n",
    "      epochs=20,\n",
    "      #validation_data=validation_generator,\n",
    "      #validation_steps=50,\n",
    "      #use_multiprocessing = True,\n",
    "      #workers = 4,\n",
    "      verbose=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jzAL69FUyG_Q"
   },
   "source": [
    "Below is the list of testing labels and their descriptions\n",
    "\n",
    "\n",
    "\n",
    "*   c0: safe driving\n",
    "*   c1: texting - right\n",
    "*   c2: talking on the phone - right\n",
    "*   c3: texting - left\n",
    "*   c4: talking on the phone - left\n",
    "*   c5: operating the radio\n",
    "*   c6: drinking\n",
    "*   c7: reaching behind\n",
    "*   c8: hair and makeup\n",
    "*   c9: talking to passenger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Code to run to save model \n",
    "model_json = model.to_json()\n",
    "with open(\"/home/jupyter/Saved_Models/model_v3.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"/home/jupyter/Saved_Models/model_v3.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "from keras import optimizers\n",
    "# Code to load json and create model\n",
    "json_file = open('/home/jupyter/Saved_Models/model_v3.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"/home/jupyter/Saved_Models/model_v3.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "loaded_model.compile(optimizer=optimizers.RMSprop(lr=1e-5),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 4, 4, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               2097408   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 16,814,666\n",
      "Trainable params: 9,179,402\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 79727 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "#test data\n",
    "base_dir = '/home/jupyter/Data'\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        '/home/jupyter/Data/TEST',\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        classes=['test'],  # only data, no labels\n",
    "        shuffle=False\n",
    "       )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79727/79727 [==============================] - 10085s 126ms/step\n",
      "10085.123625278473\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "probabilities = loaded_model.predict_generator(test_generator, 79727,verbose=1)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/home/jupyter/Submission/'\n",
    "import pandas as pd\n",
    "df_prob = pd.DataFrame(probabilities)\n",
    "df_label = pd.DataFrame(test_generator.filenames[0:79727])\n",
    "df = pd.concat([df_label,df_prob],axis=1)\n",
    "df.columns = ['img','c0','c1','c2','c3','c4','c5','c6','c7','c8','c9']\n",
    "df.head()\n",
    "df.to_csv(output_dir + 'sample_submission2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1594293"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(probabilities)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Ml1020_Pretrained_Model_V2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
