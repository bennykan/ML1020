{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/bennykan/ML1020/blob/master/Ml1020_Pretrained_Model_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===========================================================\n",
    "# ML1020 - Final Project: Mid-Term Proposal\n",
    "# Team Blue\n",
    "\n",
    "## Tyler Blakeley\n",
    "## Benjamin Kan\n",
    "## Avi Singh\n",
    "## Justin Kim\n",
    "\n",
    "\n",
    "# Distracted Driver Detection\n",
    "# ==========================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project is inspired by a past Kaggle competition hosted by State Farm, an insurance company based in the US.  Competition participants were invited to implement a machine learning algorithm to classify and predict the driverâ€™s behavior based on the images captured from the dashboard cameras installed in the vehicles (https://www.kaggle.com/c/state-farm-distracted-driver-detection/data). The host defined the following 10 driving behavior classifications\n",
    "\n",
    "| Label | Driver Behavior Descriptions |\n",
    "| ----- | ---------------------------- |\n",
    "| c0\t| normal driving\n",
    "| c1\t| texting - right\n",
    "| c2\t| talking on the phone - right\n",
    "| c3\t| texting - left\n",
    "| c4\t| talking on the phone - left\n",
    "| c5\t| operating the radio\n",
    "| c6\t| drinking\n",
    "| c7\t| reaching behind\n",
    "| c8\t| hair and makeup\n",
    "| c9\t| talking to passenger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Keras Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c69DxfPUv00z"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.version\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/ML1020\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "colab_type": "code",
    "id": "zcLWhSFg91rw",
    "outputId": "bc535b30-480c-4f60-f295-5c96ef1925ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "base_dir = '/home/jupyter/Data'\n",
    "img = cv2.imread(base_dir + '/train/c6/img_380.jpg')\n",
    "img_cvt=cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img_cvt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the VGG16 Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "MoJLVH6WDo8e",
    "outputId": "ea22c8b3-66c5-42d4-d40f-b1a20d10fb10"
   },
   "outputs": [],
   "source": [
    "#Loading in Pretrained Model\n",
    "\n",
    "from keras.applications import VGG16\n",
    "\n",
    "conv_base = VGG16(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(150, 150, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 782
    },
    "colab_type": "code",
    "id": "pGlhU7kzBMKy",
    "outputId": "b6f8028c-2857-4f85-ef17-4c7aacc66687"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 150, 150, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding One Hidden Layer on Top of VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 4, 4, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               2097408   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 16,814,666\n",
      "Trainable params: 16,814,666\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Model with Conv Base Included\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "model = models.Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu', input_dim=4 * 4 * 512))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(10, activation = 'softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "#Set conv_base training weights to false so we dont re train the weights already learned\n",
    "print(len(model.trainable_weights))\n",
    "conv_base.trainable = False\n",
    "print(len(model.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the Last Convolution Layer in VGG16 to be Re-Trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.trainable = True\n",
    "\n",
    "set_trainable = False\n",
    "for layer in conv_base.layers:\n",
    "    if layer.name == 'block5_conv1':\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "print(len(model.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data into Train and Validate Sets (80% Train % 20% Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17943 images belonging to 10 classes.\n",
      "Found 4481 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest',\n",
    "    validation_split=0.2\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # This is the target directory\n",
    "        train_dir,\n",
    "        # All images will be resized to 150x150\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='categorical',\n",
    "        subset=\"training\",\n",
    "        shuffle = True\n",
    "        )\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "        # This is the target directory\n",
    "        train_dir,\n",
    "        # All images will be resized to 150x150\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='categorical',\n",
    "        subset=\"validation\",\n",
    "        shuffle = True\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-5),\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,\n",
    "      epochs=5,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50,\n",
    "      #use_multiprocessing = True,\n",
    "      #workers = 4,\n",
    "      verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jzAL69FUyG_Q"
   },
   "source": [
    "Below is the list of testing labels and their descriptions\n",
    "\n",
    "\n",
    "\n",
    "*   c0: safe driving\n",
    "*   c1: texting - right\n",
    "*   c2: talking on the phone - right\n",
    "*   c3: texting - left\n",
    "*   c4: talking on the phone - left\n",
    "*   c5: operating the radio\n",
    "*   c6: drinking\n",
    "*   c7: reaching behind\n",
    "*   c8: hair and makeup\n",
    "*   c9: talking to passenger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Code to run to save model \n",
    "model_json = model.to_json()\n",
    "with open(\"/home/jupyter/Saved_Models/model_50Epoch_1Train_1Hidden.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"/home/jupyter/Saved_Models/model_50Epoch_1Train_1Hidden.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Training and Validation Accuracy of 50 Epoch model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model V2\n",
    "\n",
    "## Add Second Hidden Layer and Re Train the last 2 layers of VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 4, 4, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               2097408   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 16,846,282\n",
      "Trainable params: 2,131,594\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Model with Conv Base Included\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "model_v2 = models.Sequential()\n",
    "model_v2.add(conv_base)\n",
    "model_v2.add(layers.Flatten())\n",
    "model_v2.add(layers.Dense(256, activation='relu', input_dim=4 * 4 * 512))\n",
    "model_v2.add(layers.Dropout(0.5))\n",
    "model_v2.add(layers.Dense(128, activation='relu'))\n",
    "model_v2.add(layers.Dropout(0.5))\n",
    "model_v2.add(layers.Dense(10, activation = 'softmax'))\n",
    "\n",
    "\n",
    "model_v2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "conv_base.trainable = True\n",
    "\n",
    "set_trainable = False\n",
    "for layer in conv_base.layers:\n",
    "    if layer.name == 'block5_conv1' or layer.name == 'block4_conv1':\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "print(len(model_v2.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 46s - loss: 2.5232 - acc: 0.0865 - val_loss: 2.2986 - val_acc: 0.1140\n",
      "Epoch 2/100\n",
      " - 40s - loss: 2.3769 - acc: 0.0970 - val_loss: 2.2925 - val_acc: 0.1320\n",
      "Epoch 3/100\n",
      " - 40s - loss: 2.3340 - acc: 0.1060 - val_loss: 2.2839 - val_acc: 0.1240\n",
      "Epoch 4/100\n",
      " - 40s - loss: 2.3274 - acc: 0.1140 - val_loss: 2.2828 - val_acc: 0.1290\n",
      "Epoch 5/100\n",
      " - 40s - loss: 2.3131 - acc: 0.1115 - val_loss: 2.2777 - val_acc: 0.1702\n",
      "Epoch 6/100\n",
      " - 40s - loss: 2.3026 - acc: 0.1300 - val_loss: 2.2707 - val_acc: 0.1580\n",
      "Epoch 7/100\n",
      " - 39s - loss: 2.2904 - acc: 0.1260 - val_loss: 2.2511 - val_acc: 0.1930\n",
      "Epoch 8/100\n",
      " - 40s - loss: 2.2794 - acc: 0.1375 - val_loss: 2.2248 - val_acc: 0.2080\n",
      "Epoch 9/100\n",
      " - 40s - loss: 2.2436 - acc: 0.1685 - val_loss: 2.1796 - val_acc: 0.2436\n",
      "Epoch 10/100\n",
      " - 26s - loss: 2.2179 - acc: 0.1585 - val_loss: 2.1238 - val_acc: 0.2840\n",
      "Epoch 11/100\n",
      " - 26s - loss: 2.1546 - acc: 0.1970 - val_loss: 2.0756 - val_acc: 0.2930\n",
      "Epoch 12/100\n",
      " - 26s - loss: 2.1388 - acc: 0.2090 - val_loss: 2.0031 - val_acc: 0.3520\n",
      "Epoch 13/100\n",
      " - 26s - loss: 2.0929 - acc: 0.2300 - val_loss: 1.9381 - val_acc: 0.3480\n",
      "Epoch 14/100\n",
      " - 26s - loss: 2.0494 - acc: 0.2415 - val_loss: 1.9044 - val_acc: 0.3802\n",
      "Epoch 15/100\n",
      " - 26s - loss: 2.0487 - acc: 0.2490 - val_loss: 1.8954 - val_acc: 0.3680\n",
      "Epoch 16/100\n",
      " - 26s - loss: 2.0100 - acc: 0.2560 - val_loss: 1.8009 - val_acc: 0.4280\n",
      "Epoch 17/100\n",
      " - 26s - loss: 1.8985 - acc: 0.2855 - val_loss: 1.7288 - val_acc: 0.4250\n",
      "Epoch 18/100\n",
      " - 26s - loss: 1.8957 - acc: 0.3013 - val_loss: 1.6470 - val_acc: 0.4760\n",
      "Epoch 19/100\n",
      " - 26s - loss: 1.8387 - acc: 0.3255 - val_loss: 1.6327 - val_acc: 0.4720\n",
      "Epoch 20/100\n",
      " - 26s - loss: 1.7819 - acc: 0.3410 - val_loss: 1.5398 - val_acc: 0.5460\n",
      "Epoch 21/100\n",
      " - 26s - loss: 1.7372 - acc: 0.3650 - val_loss: 1.4460 - val_acc: 0.5720\n",
      "Epoch 22/100\n",
      " - 26s - loss: 1.6673 - acc: 0.4090 - val_loss: 1.3958 - val_acc: 0.5780\n",
      "Epoch 23/100\n",
      " - 26s - loss: 1.6994 - acc: 0.3880 - val_loss: 1.3681 - val_acc: 0.5892\n",
      "Epoch 24/100\n",
      " - 26s - loss: 1.6043 - acc: 0.4270 - val_loss: 1.3033 - val_acc: 0.6180\n",
      "Epoch 25/100\n",
      " - 26s - loss: 1.5387 - acc: 0.4555 - val_loss: 1.1879 - val_acc: 0.6160\n",
      "Epoch 26/100\n",
      " - 26s - loss: 1.4730 - acc: 0.4840 - val_loss: 1.1214 - val_acc: 0.6650\n",
      "Epoch 27/100\n",
      " - 26s - loss: 1.4214 - acc: 0.5127 - val_loss: 1.0786 - val_acc: 0.6453\n",
      "Epoch 28/100\n",
      " - 26s - loss: 1.3943 - acc: 0.5060 - val_loss: 1.0241 - val_acc: 0.7050\n",
      "Epoch 29/100\n",
      " - 27s - loss: 1.3426 - acc: 0.5430 - val_loss: 0.9893 - val_acc: 0.6990\n",
      "Epoch 30/100\n",
      " - 26s - loss: 1.2888 - acc: 0.5465 - val_loss: 1.0131 - val_acc: 0.6990\n",
      "Epoch 31/100\n",
      " - 27s - loss: 1.2153 - acc: 0.5755 - val_loss: 0.8477 - val_acc: 0.7560\n",
      "Epoch 32/100\n",
      " - 26s - loss: 1.2022 - acc: 0.6005 - val_loss: 0.8450 - val_acc: 0.7207\n",
      "Epoch 33/100\n",
      " - 26s - loss: 1.1355 - acc: 0.6035 - val_loss: 0.7910 - val_acc: 0.7620\n",
      "Epoch 34/100\n",
      " - 26s - loss: 1.0958 - acc: 0.6305 - val_loss: 0.7609 - val_acc: 0.7510\n",
      "Epoch 35/100\n",
      " - 26s - loss: 1.0549 - acc: 0.6385 - val_loss: 0.7830 - val_acc: 0.7820\n",
      "Epoch 36/100\n",
      " - 26s - loss: 1.0715 - acc: 0.6442 - val_loss: 0.7027 - val_acc: 0.8033\n",
      "Epoch 37/100\n",
      " - 26s - loss: 0.9656 - acc: 0.6720 - val_loss: 0.6961 - val_acc: 0.7920\n",
      "Epoch 38/100\n",
      " - 26s - loss: 0.9355 - acc: 0.6845 - val_loss: 0.6222 - val_acc: 0.8110\n",
      "Epoch 39/100\n",
      " - 27s - loss: 0.9243 - acc: 0.6860 - val_loss: 0.7033 - val_acc: 0.7720\n",
      "Epoch 40/100\n",
      " - 27s - loss: 0.8615 - acc: 0.7045 - val_loss: 0.7829 - val_acc: 0.7440\n",
      "Epoch 41/100\n",
      " - 26s - loss: 0.8815 - acc: 0.6950 - val_loss: 0.5783 - val_acc: 0.8257\n",
      "Epoch 42/100\n",
      " - 26s - loss: 0.8244 - acc: 0.7235 - val_loss: 0.4755 - val_acc: 0.8600\n",
      "Epoch 43/100\n",
      " - 26s - loss: 0.7991 - acc: 0.7350 - val_loss: 0.4697 - val_acc: 0.8650\n",
      "Epoch 44/100\n",
      " - 26s - loss: 0.8018 - acc: 0.7205 - val_loss: 0.4889 - val_acc: 0.8550\n",
      "Epoch 45/100\n",
      " - 26s - loss: 0.7708 - acc: 0.7409 - val_loss: 0.4459 - val_acc: 0.8563\n",
      "Epoch 46/100\n",
      " - 26s - loss: 0.7206 - acc: 0.7725 - val_loss: 0.5151 - val_acc: 0.8380\n",
      "Epoch 47/100\n",
      " - 26s - loss: 0.6932 - acc: 0.7670 - val_loss: 0.3970 - val_acc: 0.8780\n",
      "Epoch 48/100\n",
      " - 26s - loss: 0.6534 - acc: 0.7775 - val_loss: 0.3914 - val_acc: 0.8820\n",
      "Epoch 49/100\n",
      " - 26s - loss: 0.6684 - acc: 0.7805 - val_loss: 0.4072 - val_acc: 0.8710\n",
      "Epoch 50/100\n",
      " - 27s - loss: 0.6293 - acc: 0.7965 - val_loss: 0.3352 - val_acc: 0.8960\n",
      "Epoch 51/100\n",
      " - 26s - loss: 0.6513 - acc: 0.7790 - val_loss: 0.3896 - val_acc: 0.8810\n",
      "Epoch 52/100\n",
      " - 26s - loss: 0.6243 - acc: 0.7940 - val_loss: 0.3112 - val_acc: 0.9100\n",
      "Epoch 53/100\n",
      " - 26s - loss: 0.5886 - acc: 0.7985 - val_loss: 0.3128 - val_acc: 0.9090\n",
      "Epoch 54/100\n",
      " - 26s - loss: 0.5851 - acc: 0.8150 - val_loss: 0.3644 - val_acc: 0.8950\n",
      "Epoch 55/100\n",
      " - 26s - loss: 0.5396 - acc: 0.8175 - val_loss: 0.4597 - val_acc: 0.8470\n",
      "Epoch 56/100\n",
      " - 26s - loss: 0.5528 - acc: 0.8100 - val_loss: 0.3028 - val_acc: 0.9000\n",
      "Epoch 57/100\n",
      " - 26s - loss: 0.5152 - acc: 0.8340 - val_loss: 0.2651 - val_acc: 0.9130\n",
      "Epoch 58/100\n",
      " - 26s - loss: 0.4927 - acc: 0.8460 - val_loss: 0.2553 - val_acc: 0.9280\n",
      "Epoch 59/100\n",
      " - 26s - loss: 0.4947 - acc: 0.8460 - val_loss: 0.2875 - val_acc: 0.9174\n",
      "Epoch 60/100\n",
      " - 26s - loss: 0.5125 - acc: 0.8445 - val_loss: 0.2725 - val_acc: 0.9140\n",
      "Epoch 61/100\n",
      " - 26s - loss: 0.4486 - acc: 0.8595 - val_loss: 0.2783 - val_acc: 0.9240\n",
      "Epoch 62/100\n",
      " - 26s - loss: 0.4684 - acc: 0.8625 - val_loss: 0.2540 - val_acc: 0.9180\n",
      "Epoch 63/100\n",
      " - 26s - loss: 0.4498 - acc: 0.8657 - val_loss: 0.3089 - val_acc: 0.9093\n",
      "Epoch 64/100\n",
      " - 26s - loss: 0.4068 - acc: 0.8730 - val_loss: 0.2073 - val_acc: 0.9370\n",
      "Epoch 65/100\n",
      " - 26s - loss: 0.3915 - acc: 0.8845 - val_loss: 0.2346 - val_acc: 0.9340\n",
      "Epoch 66/100\n",
      " - 26s - loss: 0.4051 - acc: 0.8735 - val_loss: 0.3408 - val_acc: 0.8890\n",
      "Epoch 67/100\n",
      " - 26s - loss: 0.4087 - acc: 0.8790 - val_loss: 0.2164 - val_acc: 0.9410\n",
      "Epoch 68/100\n",
      " - 26s - loss: 0.4065 - acc: 0.8770 - val_loss: 0.2486 - val_acc: 0.9276\n",
      "Epoch 69/100\n",
      " - 26s - loss: 0.3886 - acc: 0.8815 - val_loss: 0.2508 - val_acc: 0.9270\n",
      "Epoch 70/100\n",
      " - 26s - loss: 0.3880 - acc: 0.8815 - val_loss: 0.1767 - val_acc: 0.9460\n",
      "Epoch 71/100\n",
      " - 26s - loss: 0.3734 - acc: 0.8890 - val_loss: 0.2668 - val_acc: 0.9280\n",
      "Epoch 72/100\n",
      " - 26s - loss: 0.3591 - acc: 0.8870 - val_loss: 0.1734 - val_acc: 0.9409\n",
      "Epoch 73/100\n",
      " - 26s - loss: 0.3442 - acc: 0.8960 - val_loss: 0.2610 - val_acc: 0.9210\n",
      "Epoch 74/100\n",
      " - 26s - loss: 0.3601 - acc: 0.8895 - val_loss: 0.1612 - val_acc: 0.9530\n",
      "Epoch 75/100\n",
      " - 27s - loss: 0.3704 - acc: 0.9000 - val_loss: 0.1506 - val_acc: 0.9560\n",
      "Epoch 76/100\n",
      " - 26s - loss: 0.3109 - acc: 0.8990 - val_loss: 0.1933 - val_acc: 0.9430\n",
      "Epoch 77/100\n",
      " - 26s - loss: 0.3477 - acc: 0.8885 - val_loss: 0.1675 - val_acc: 0.9572\n",
      "Epoch 78/100\n",
      " - 26s - loss: 0.3237 - acc: 0.8985 - val_loss: 0.1684 - val_acc: 0.9580\n",
      "Epoch 79/100\n",
      " - 26s - loss: 0.2929 - acc: 0.9120 - val_loss: 0.1707 - val_acc: 0.9530\n",
      "Epoch 80/100\n",
      " - 26s - loss: 0.3052 - acc: 0.9100 - val_loss: 0.1422 - val_acc: 0.9580\n",
      "Epoch 81/100\n",
      " - 26s - loss: 0.3194 - acc: 0.9092 - val_loss: 0.1973 - val_acc: 0.9399\n",
      "Epoch 82/100\n",
      " - 26s - loss: 0.2680 - acc: 0.9205 - val_loss: 0.1562 - val_acc: 0.9600\n",
      "Epoch 83/100\n",
      " - 26s - loss: 0.3275 - acc: 0.9025 - val_loss: 0.1291 - val_acc: 0.9620\n",
      "Epoch 84/100\n",
      " - 26s - loss: 0.2771 - acc: 0.9180 - val_loss: 0.1988 - val_acc: 0.9440\n",
      "Epoch 85/100\n",
      " - 27s - loss: 0.2718 - acc: 0.9220 - val_loss: 0.1626 - val_acc: 0.9500\n",
      "Epoch 86/100\n",
      " - 26s - loss: 0.3240 - acc: 0.9045 - val_loss: 0.1629 - val_acc: 0.9450\n",
      "Epoch 87/100\n",
      " - 27s - loss: 0.2494 - acc: 0.9235 - val_loss: 0.1940 - val_acc: 0.9520\n",
      "Epoch 88/100\n",
      " - 26s - loss: 0.2548 - acc: 0.9245 - val_loss: 0.1613 - val_acc: 0.9480\n",
      "Epoch 89/100\n",
      " - 26s - loss: 0.2496 - acc: 0.9275 - val_loss: 0.2298 - val_acc: 0.9430\n",
      "Epoch 90/100\n",
      " - 26s - loss: 0.2663 - acc: 0.9189 - val_loss: 0.2010 - val_acc: 0.9419\n",
      "Epoch 91/100\n",
      " - 26s - loss: 0.2668 - acc: 0.9245 - val_loss: 0.1663 - val_acc: 0.9540\n",
      "Epoch 92/100\n",
      " - 26s - loss: 0.2636 - acc: 0.9285 - val_loss: 0.2090 - val_acc: 0.9460\n",
      "Epoch 93/100\n",
      " - 26s - loss: 0.2332 - acc: 0.9275 - val_loss: 0.1682 - val_acc: 0.9430\n",
      "Epoch 94/100\n",
      " - 26s - loss: 0.2337 - acc: 0.9305 - val_loss: 0.1528 - val_acc: 0.9570\n",
      "Epoch 95/100\n",
      " - 26s - loss: 0.2551 - acc: 0.9360 - val_loss: 0.1827 - val_acc: 0.9439\n",
      "Epoch 96/100\n",
      " - 26s - loss: 0.2260 - acc: 0.9410 - val_loss: 0.1842 - val_acc: 0.9460\n",
      "Epoch 97/100\n",
      " - 27s - loss: 0.2460 - acc: 0.9315 - val_loss: 0.1812 - val_acc: 0.9460\n",
      "Epoch 98/100\n",
      " - 27s - loss: 0.1982 - acc: 0.9490 - val_loss: 0.0782 - val_acc: 0.9760\n",
      "Epoch 99/100\n",
      " - 26s - loss: 0.2098 - acc: 0.9365 - val_loss: 0.1629 - val_acc: 0.9531\n",
      "Epoch 100/100\n",
      " - 26s - loss: 0.2234 - acc: 0.9425 - val_loss: 0.1564 - val_acc: 0.9630\n"
     ]
    }
   ],
   "source": [
    "model_v2.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-5),\n",
    "              metrics=['acc'])\n",
    "\n",
    "history_v2 = model_v2.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,\n",
    "      epochs=100,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50,\n",
    "      #use_multiprocessing = True,\n",
    "      #workers = 4,\n",
    "      verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "from keras import optimizers\n",
    "# Code to load json and create model\n",
    "json_file = open('/home/jupyter/Saved_Models/model_100epoch_2CNN_2hidden.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"/home/jupyter/Saved_Models/model_100epoch_2CNN_2hidden.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "loaded_model.compile(optimizer=optimizers.RMSprop(lr=1e-5),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /home/jupyter/Saved_Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = model_v2.to_json()\n",
    "with open(\"/home/jupyter/Saved_Models/model_100epoch_2CNN_2hidden.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model_v2.save_weights(\"/home/jupyter/Saved_Models/model_100epoch_2CNN_2hidden.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 79727 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "#test data\n",
    "base_dir = '/home/jupyter/Data'\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        '/home/jupyter/Data/TEST',\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        classes=['test'],  # only data, no labels\n",
    "        shuffle=False\n",
    "       )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3987/3987 [==============================] - 489s 123ms/step\n",
      "489.02042961120605\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "probabilities = loaded_model.predict_generator(test_generator, len(test_generator),verbose=1)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/home/jupyter/Submission/'\n",
    "import pandas as pd\n",
    "df_prob = pd.DataFrame(probabilities)\n",
    "df_label = pd.DataFrame(test_generator.filenames[0:79727])\n",
    "df = pd.concat([df_label,df_prob],axis=1)\n",
    "df.columns = ['img','c0','c1','c2','c3','c4','c5','c6','c7','c8','c9']\n",
    "df.head()\n",
    "df.to_csv(output_dir + 'sample_submission6.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79727"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79727, 11)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Ml1020_Pretrained_Model_V2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
